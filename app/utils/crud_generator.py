# app/utils/crud.py
"""
Fonctions CRUD r√©utilisables pour g√©rer les ressources JSON.

Usage dans les routes :
    from utils.crud import list_all, get_one, create_one, update_one, delete_one
    
    @router.get("/")
    def list_professions():
        return list_all(config.PROFESSIONS_FILE, "professions", logger)
"""

from pathlib import Path
from typing import Optional, Dict, Any, List
from fastapi import HTTPException
from utils.json import load_json, save_json
import logging

# ============================================================================
# READ Operations
# ============================================================================

def list_all(
    file_path: Path,
    resource_name: str,
    logger: logging.Logger,
    user_id: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Liste tous les √©l√©ments d'une ressource.
    
    Args:
        file_path: Chemin du fichier JSON
        resource_name: Nom de la ressource (pour les logs)
        logger: Logger √† utiliser
        user_id: ID de l'utilisateur (optionnel, pour les logs)
    
    Returns:
        Liste des √©l√©ments
    
    Raises:
        HTTPException: 500 si erreur de lecture
    
    Example:
        items = list_all(config.PROFESSIONS_FILE, "professions", logger)
    """
    log_prefix = f"user_id={user_id}" if user_id else "public"
    logger.info(f"üìã Liste des {resource_name} ({log_prefix})")
    
    try:
        data = load_json(file_path) or {}
        items = list(data.values())
        logger.debug(f"   ‚Üí {len(items)} {resource_name} trouv√©(s)")
        return items
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des {resource_name}", exc_info=True)
        raise HTTPException(500, f"Failed to retrieve {resource_name}")


def get_one(
    file_path: Path,
    item_id: str,
    resource_name: str,
    logger: logging.Logger,
    user_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    R√©cup√®re un √©l√©ment par son ID.
    
    Args:
        file_path: Chemin du fichier JSON
        item_id: ID de l'√©l√©ment √† r√©cup√©rer
        resource_name: Nom de la ressource (pour les logs)
        logger: Logger √† utiliser
        user_id: ID de l'utilisateur (optionnel, pour les logs)
    
    Returns:
        √âl√©ment trouv√©
    
    Raises:
        HTTPException: 404 si non trouv√©, 500 si erreur
    
    Example:
        profession = get_one(config.PROFESSIONS_FILE, "mineur", "profession", logger)
    """
    log_prefix = f"user_id={user_id}" if user_id else "public"
    logger.info(f"üîç R√©cup√©ration {resource_name} '{item_id}' ({log_prefix})")
    
    try:
        data = load_json(file_path) or {}
        
        if item_id not in data:
            logger.warning(f"‚ö†Ô∏è  {resource_name.capitalize()} '{item_id}' non trouv√©")
            raise HTTPException(404, f"{resource_name.capitalize()} not found")
        
        item = data[item_id]
        logger.debug(f"   ‚Üí {resource_name.capitalize()} '{item_id}' r√©cup√©r√©")
        return item
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration du {resource_name}", exc_info=True)
        raise HTTPException(500, f"Failed to retrieve {resource_name}")


# ============================================================================
# CREATE Operation
# ============================================================================

def create_one(
    file_path: Path,
    payload: Dict[str, Any],
    resource_name: str,
    logger: logging.Logger,
    id_field: str = "id"
) -> Dict[str, Any]:
    """
    Cr√©e un nouvel √©l√©ment.
    
    Args:
        file_path: Chemin du fichier JSON
        payload: Donn√©es de l'√©l√©ment √† cr√©er
        resource_name: Nom de la ressource (pour les logs)
        logger: Logger √† utiliser
        id_field: Nom du champ ID (par d√©faut "id")
    
    Returns:
        √âl√©ment cr√©√©
    
    Raises:
        HTTPException: 400 si ID manquant ou existe d√©j√†, 500 si erreur
    
    Example:
        profession = create_one(
            config.PROFESSIONS_FILE,
            {"id": "mineur", "name": "Mineur"},
            "profession",
            logger
        )
    """
    item_id = payload.get(id_field)
    
    if not item_id:
        logger.warning(f"‚ö†Ô∏è  Tentative de cr√©ation sans {id_field}")
        raise HTTPException(400, f"{id_field} is required")
    
    logger.info(f"‚ûï Cr√©ation {resource_name} '{item_id}'")
    
    try:
        data = load_json(file_path) or {}
        
        if item_id in data:
            logger.warning(f"‚ö†Ô∏è  {resource_name.capitalize()} '{item_id}' existe d√©j√†")
            raise HTTPException(400, f"{resource_name.capitalize()} already exists")
        
        data[item_id] = payload
        save_json(file_path, data)
        
        logger.info(f"‚úÖ {resource_name.capitalize()} '{item_id}' cr√©√© avec succ√®s")
        return payload
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation du {resource_name}", exc_info=True)
        raise HTTPException(500, f"Failed to create {resource_name}")


# ============================================================================
# UPDATE Operation
# ============================================================================

def update_one(
    file_path: Path,
    item_id: str,
    payload: Dict[str, Any],
    resource_name: str,
    logger: logging.Logger,
    merge: bool = True
) -> Dict[str, Any]:
    """
    Met √† jour un √©l√©ment existant.
    
    Args:
        file_path: Chemin du fichier JSON
        item_id: ID de l'√©l√©ment √† mettre √† jour
        payload: Nouvelles donn√©es
        resource_name: Nom de la ressource (pour les logs)
        logger: Logger √† utiliser
        merge: Si True, fusionne avec l'existant. Si False, remplace compl√®tement.
    
    Returns:
        √âl√©ment mis √† jour
    
    Raises:
        HTTPException: 404 si non trouv√©, 500 si erreur
    
    Example:
        updated = update_one(
            config.PROFESSIONS_FILE,
            "mineur",
            {"name": "Grand Mineur"},
            "profession",
            logger
        )
    """
    logger.info(f"‚úèÔ∏è  Mise √† jour {resource_name} '{item_id}'")
    logger.debug(f"   ‚Üí Champs: {list(payload.keys())}")
    
    try:
        data = load_json(file_path) or {}
        
        if item_id not in data:
            logger.warning(f"‚ö†Ô∏è  {resource_name.capitalize()} '{item_id}' non trouv√©")
            raise HTTPException(404, f"{resource_name.capitalize()} not found")
        
        if merge:
            # Fusion avec l'existant
            data[item_id].update(payload)
        else:
            # Remplacement complet
            data[item_id] = payload
        
        save_json(file_path, data)
        
        logger.info(f"‚úÖ {resource_name.capitalize()} '{item_id}' mis √† jour")
        return data[item_id]
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la mise √† jour du {resource_name}", exc_info=True)
        raise HTTPException(500, f"Failed to update {resource_name}")


# ============================================================================
# DELETE Operation
# ============================================================================

def delete_one(
    file_path: Path,
    item_id: str,
    resource_name: str,
    logger: logging.Logger
) -> Dict[str, Any]:
    """
    Supprime un √©l√©ment.
    
    Args:
        file_path: Chemin du fichier JSON
        item_id: ID de l'√©l√©ment √† supprimer
        resource_name: Nom de la ressource (pour les logs)
        logger: Logger √† utiliser
    
    Returns:
        Dict avec status et id de l'√©l√©ment supprim√©
    
    Raises:
        HTTPException: 404 si non trouv√©, 500 si erreur
    
    Example:
        result = delete_one(config.PROFESSIONS_FILE, "mineur", "profession", logger)
    """
    logger.info(f"üóëÔ∏è  Suppression {resource_name} '{item_id}'")
    
    try:
        data = load_json(file_path) or {}
        
        if item_id not in data:
            logger.warning(f"‚ö†Ô∏è  {resource_name.capitalize()} '{item_id}' non trouv√©")
            raise HTTPException(404, f"{resource_name.capitalize()} not found")
        
        del data[item_id]
        save_json(file_path, data)
        
        logger.info(f"‚úÖ {resource_name.capitalize()} '{item_id}' supprim√©")
        return {"status": "deleted", "id": item_id}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la suppression du {resource_name}", exc_info=True)
        raise HTTPException(500, f"Failed to delete {resource_name}")


# ============================================================================
# BULK Operations (bonus)
# ============================================================================

def bulk_create(
    file_path: Path,
    items: List[Dict[str, Any]],
    resource_name: str,
    logger: logging.Logger,
    id_field: str = "id",
    skip_existing: bool = False
) -> Dict[str, Any]:
    """
    Cr√©e plusieurs √©l√©ments en une seule op√©ration.
    
    Args:
        file_path: Chemin du fichier JSON
        items: Liste des √©l√©ments √† cr√©er
        resource_name: Nom de la ressource (pour les logs)
        logger: Logger √† utiliser
        id_field: Nom du champ ID
        skip_existing: Si True, ignore les doublons. Si False, erreur sur doublon.
    
    Returns:
        Dict avec le nombre cr√©√©/ignor√©/erreurs
    
    Example:
        result = bulk_create(
            config.PROFESSIONS_FILE,
            [{"id": "mineur", ...}, {"id": "bucheron", ...}],
            "professions",
            logger
        )
    """
    logger.info(f"‚ûï Cr√©ation en masse de {len(items)} {resource_name}")
    
    try:
        data = load_json(file_path) or {}
        
        created = 0
        skipped = 0
        errors = []
        
        for item in items:
            item_id = item.get(id_field)
            
            if not item_id:
                errors.append(f"Missing {id_field}")
                continue
            
            if item_id in data:
                if skip_existing:
                    logger.debug(f"   ‚Üí {item_id} existe d√©j√†, ignor√©")
                    skipped += 1
                    continue
                else:
                    errors.append(f"{item_id} already exists")
                    continue
            
            data[item_id] = item
            created += 1
            logger.debug(f"   ‚Üí {item_id} cr√©√©")
        
        if created > 0:
            save_json(file_path, data)
        
        logger.info(f"‚úÖ Cr√©ation en masse termin√©e: {created} cr√©√©s, {skipped} ignor√©s, {len(errors)} erreurs")
        
        return {
            "created": created,
            "skipped": skipped,
            "errors": errors
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation en masse", exc_info=True)
        raise HTTPException(500, f"Failed to bulk create {resource_name}")